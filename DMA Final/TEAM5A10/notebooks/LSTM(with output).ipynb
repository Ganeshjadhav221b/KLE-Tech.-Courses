{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "#import matplotlib as mpl\n",
    "#mpl.style.use('ggplot')\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "# math and data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# to handle paths\n",
    "from pathlib import Path\n",
    "\n",
    "# set random seeds \n",
    "from numpy.random import seed\n",
    "from tensorflow import set_random_seed\n",
    "\n",
    "RANDOM_SEED = 2018\n",
    "seed(RANDOM_SEED)\n",
    "set_random_seed(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"numpy_path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA PREPROCESSING (Date time parsing and normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [1]Date Time Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'consumption_train.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-68733f63595a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'consumption_train.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'timestamp'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cold_start_test.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'timestamp'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msub_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'submission_format.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pred_id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'timestamp'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"series_id\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"timestamp\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"consumption\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"temperature\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda4\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda4\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda4\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda4\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1015\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda4\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1708\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'consumption_train.csv' does not exist"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('consumption_train.csv',index_col=0, parse_dates=['timestamp'])\n",
    "test_data = pd.read_csv('cold_start_test.csv',index_col=0, parse_dates=['timestamp'])\n",
    "sub_data = pd.read_csv('submission_format.csv',index_col='pred_id',parse_dates=['timestamp'])\n",
    "\n",
    "train_data = train_data.loc[:,[\"series_id\",\"timestamp\",\"consumption\",\"temperature\"]] \n",
    "test_data = test_data.loc[:,['series_id','timestamp','consumption','temperature']]\n",
    "\n",
    "sub_data = sub_data.loc[:,['series_id','timestamp','temperature','consumption','prediction_window']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [2]Creating Lagged Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lagged_features(df, lag=1):\n",
    "    if not type(df) == pd.DataFrame:\n",
    "        df = pd.DataFrame(df, columns=['consumption'])\n",
    "    \n",
    "    def _rename_lag(ser, j):\n",
    "        ser.name = ser.name + f'_{j}'\n",
    "        return ser\n",
    "        \n",
    "    # add a column lagged by `i` steps\n",
    "    for i in range(1, lag + 1):\n",
    "        df = df.join(df.consumption.shift(i).pipe(_rename_lag, i))\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "    return df\n",
    "\n",
    "# example series\n",
    "test_series = train_data[train_data.series_id == 100283]\n",
    "create_lagged_features(test_series.consumption, lag=3).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [3]Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def prepare_training_data(consumption_series, lag):\n",
    "    \"\"\" Converts a series of consumption data into a\n",
    "        lagged, scaled sample.\n",
    "    \"\"\"\n",
    "    # scale training data\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    consumption_vals = scaler.fit_transform(consumption_series.values.reshape(-1, 1))\n",
    "    \n",
    "    # convert consumption series to lagged features\n",
    "    consumption_lagged = create_lagged_features(consumption_vals, lag=lag)\n",
    "\n",
    "    # X, y format taking the first column (original time series) to be the y\n",
    "    X = consumption_lagged.drop('consumption', axis=1).values\n",
    "    y = consumption_lagged.consumption.values\n",
    "    \n",
    "    # keras expects 3 dimensional X\n",
    "    X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "    \n",
    "    return X, y, scaler\n",
    "\n",
    "_X, _y, scaler = prepare_training_data(test_series.consumption, 5)\n",
    "print(_X.shape)\n",
    "print(_y.shape)\n",
    "print(scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modeling\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "# progress bar\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long short-term memory (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LSTM units are units of a recurrent neural network (RNN)  well-suited to classifying, processing and making predictions based on time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lag of 24 to simulate smallest cold start window. Our series\n",
    "# will be converted to a num_timesteps x lag size matrix\n",
    "lag =  24\n",
    "\n",
    "# model parameters\n",
    "num_neurons = 24\n",
    "batch_size = 1  # this for`ces the lstm to step through each time-step one at a time\n",
    "batch_input_shape=(batch_size, 1, lag)\n",
    "\n",
    "# instantiate a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# add LSTM layer - stateful MUST be true here in \n",
    "# order to learn the patterns within a series\n",
    "model.add(LSTM(units=num_neurons, \n",
    "              batch_input_shape=batch_input_shape, \n",
    "              stateful=True))\n",
    "\n",
    "# followed by a dense layer with a single output for regression\n",
    "model.add(Dense(1))\n",
    "\n",
    "# compile\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "num_training_series = train_data.series_id.nunique()\n",
    "num_passes_through_data = 3\n",
    "\n",
    "for i in tqdm(range(num_passes_through_data), \n",
    "              total=num_passes_through_data, \n",
    "              desc='Learning Consumption Trends - Epoch'):\n",
    "    \n",
    "    # reset the LSTM state for training on each series\n",
    "    for ser_id, ser_data in train_data.groupby('series_id'):\n",
    "\n",
    "        # prepare the data\n",
    "        X, y, scaler = prepare_training_data(ser_data.consumption, lag)\n",
    "\n",
    "        # fit the model: note that we don't shuffle batches (it would ruin the sequence)\n",
    "        # and that we reset states only after an entire X has been fit, instead of after\n",
    "        # each (size 1) batch, as is the case when stateful=False\n",
    "        model.fit(X, y, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)\n",
    "        model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hourly_forecast(num_pred_hours, consumption, model, scaler, lag):\n",
    "    \"\"\" Uses last hour's prediction to generate next for num_pred_hours, \n",
    "        initialized by most recent cold start prediction. Inverts scale of \n",
    "        predictions before return.\n",
    "    \"\"\"\n",
    "    # allocate prediction frame\n",
    "    preds_scaled = np.zeros(num_pred_hours)\n",
    "    \n",
    "    # initial X is last lag values from the cold start\n",
    "    X = scaler.transform(consumption.values.reshape(-1, 1))[-lag:]\n",
    "    \n",
    "    # forecast\n",
    "    for i in range(num_pred_hours):\n",
    "        # predict scaled value for next time step\n",
    "        yhat = model.predict(X.reshape(1, 1, lag), batch_size=1)[0][0]\n",
    "        preds_scaled[i] = yhat\n",
    "        \n",
    "        # update X to be latest data plus prediction\n",
    "        X = pd.Series(X.ravel()).shift(-1).fillna(yhat).values\n",
    "\n",
    "    # revert scale back to original range\n",
    "    hourly_preds = scaler.inverse_transform(preds_scaled.reshape(-1, 1)).ravel()\n",
    "    return hourly_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1]Predicting for submission format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy submission format and fill in values\n",
    "my_submission = sub_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytest = test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%time\n",
    "pred_window_to_num_preds = {'hourly': 24, 'daily': 7, 'weekly': 2}\n",
    "pred_window_to_num_pred_hours = {'hourly': 24, 'daily': 7 * 24, 'weekly': 2 * 7 * 24}\n",
    "\n",
    "num_test_series = my_submission.series_id.nunique()\n",
    "\n",
    "model.reset_states()\n",
    "\n",
    "for ser_id, pred_df in tqdm(my_submission.groupby('series_id'), \n",
    "                            total=num_test_series, \n",
    "                            desc=\"Forecasting from Cold Start Data\"):\n",
    "        \n",
    "    # get info about this series' prediction window\n",
    "    pred_window = pred_df.prediction_window.unique()[0]\n",
    "    num_preds = pred_window_to_num_preds[pred_window]\n",
    "    num_pred_hours = pred_window_to_num_pred_hours[pred_window]\n",
    "    \n",
    "    # prepare cold start data\n",
    "    series_data = test_data[test_data.series_id == ser_id].consumption\n",
    "    cold_X, cold_y, scaler = prepare_training_data(series_data, lag)\n",
    "    \n",
    "    # fine tune our lstm model to this site using cold start data    \n",
    "    model.fit(cold_X, cold_y, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)\n",
    "    \n",
    "    # make hourly forecasts for duration of pred window\n",
    "    preds = generate_hourly_forecast(num_pred_hours, series_data, model, scaler, lag)\n",
    "    \n",
    "    # reduce by taking sum over each sub window in pred window\n",
    "    reduced_preds = [pred.sum() for pred in np.split(preds, num_preds)]\n",
    "    \n",
    "    # store result in submission DataFrame\n",
    "    ser_id_mask = my_submission.series_id == ser_id\n",
    "    my_submission.loc[ser_id_mask, 'consumption'] = reduced_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_submission[my_submission.prediction_window == 'hourly'].consumption.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_submission.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_submission.to_csv(\"my_submmission.csv\", index_label='pred_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [2]Predicting for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%time\n",
    "pred_window_to_num_preds = {'hourly': 24, 'daily': 7, 'weekly': 2}\n",
    "pred_window_to_num_pred_hours = {'hourly': 24, 'daily': 7 * 24, 'weekly': 2 * 7 * 24}\n",
    "\n",
    "num_test_series = mytest.series_id.nunique()\n",
    "\n",
    "model.reset_states()\n",
    "\n",
    "for ser_id, pred_df in tqdm(mytest.groupby('series_id'), \n",
    "                            total=num_test_series, \n",
    "                            desc=\"Forecasting from Cold Start Data\"):\n",
    "        \n",
    "    # get info about this series' prediction window\n",
    "    pred_window = pred_df.prediction_window.unique()[0]\n",
    "    num_preds = 1\n",
    "    num_pred_hours = 1\n",
    "    \n",
    "    # prepare cold start data\n",
    "    series_data = test_data[test_data.series_id == ser_id].consumption\n",
    "    cold_X, cold_y, scaler = prepare_training_data(series_data, lag)\n",
    "    \n",
    "    # fine tune our lstm model to this site using cold start data    \n",
    "    model.fit(cold_X, cold_y, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)\n",
    "    \n",
    "    # make hourly forecasts for duration of pred window\n",
    "    preds = generate_hourly_forecast(1, series_data, model, scaler, lag)\n",
    "    \n",
    "    # reduce by taking sum over each sub window in pred window\n",
    "    #reduced_preds = [pred.sum() for pred in np.split(preds, num_preds)]\n",
    "    \n",
    "    # store result in submission DataFrame\n",
    "    ser_id_mask = mytest.series_id == ser_id\n",
    "    mytest.loc[ser_id_mask, 'consumption'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytest.consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Accuracy for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(y1, y0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [2]Predicting on Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytrain=train_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for train\n",
    "#%%time\n",
    "pred_window_to_num_preds = {'hourly': 24, 'daily': 7, 'weekly': 2}\n",
    "pred_window_to_num_pred_hours = {'hourly': 24, 'daily': 7 * 24, 'weekly': 2 * 7 * 24}\n",
    "\n",
    "num_test_series = mytrain.series_id.nunique()\n",
    "\n",
    "model.reset_states()\n",
    "\n",
    "for ser_id, pred_df in tqdm(mytrain.groupby('series_id'), \n",
    "                            total=num_test_series, \n",
    "                            desc=\"Forecasting from Cold Start Data\"):\n",
    "        \n",
    "    # get info about this series' prediction window\n",
    "    #pred_window = pred_df.prediction_window.unique()[0]\n",
    "    num_preds = 1\n",
    "    num_pred_hours = 1\n",
    "    \n",
    "    # prepare cold start data\n",
    "    series_data = train_data[train_data.series_id == ser_id].consumption\n",
    "    cold_X, cold_y, scaler = prepare_training_data(series_data, lag)\n",
    "    \n",
    "    # fine tune our lstm model to this site using cold start data    \n",
    "    model.fit(cold_X, cold_y, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)\n",
    "    \n",
    "    # make hourly forecasts for duration of pred window\n",
    "    preds = generate_hourly_forecast(1, series_data, model, scaler, lag)\n",
    "    \n",
    "    # reduce by taking sum over each sub window in pred window\n",
    "    #reduced_preds = [pred.sum() for pred in np.split(preds, num_preds)]\n",
    "    \n",
    "    # store result in submission DataFrame\n",
    "    ser_id_mask = mytrain.series_id == ser_id\n",
    "    mytrain.loc[ser_id_mask, 'consumption'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy0=mytrain.consumption\n",
    "\n",
    "yy1=train_data.consumption\n",
    "print(yy0)\n",
    "print(yy1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Accuracy for Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(yy1, yy0) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
